geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Frequency of Sentiment Scores - NRC",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
(g1|g2)
c1 <- ggplot(sentences_rep_cnr, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores - CNR",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
# Second plot
c2 <- ggplot(sentences_rep_nrc, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores - NRC",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
(c1|c2)
sentences_female_nrc <- calculate_sentiment_score(data[data$final_gender == "female",], Rep_Emoji, nrc_lex )
sentences_male_nrc <- calculate_sentiment_score(data[data$final_gender == "male",], Rep_Emoji, nrc_lex )
#| fig-cap: "Sentences Sentiment in the Female Users with NRC"
ff1 <- ggplot(sentences_female_nrc, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores - CNR",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
# Second plot
ff2 <- ggplot(sentences_male_nrc, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores - NRC",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
(ff1|ff2)
sentences_female_cnr <- calculate_sentiment_score(data[data$final_gender == "female",], Rep_Emoji, cnr_lex )
sentences_male_cnr <- calculate_sentiment_score(data[data$final_gender == "male",], Rep_Emoji, cnr_lex )
#| fig-cap: "Sentences Sentiment between Genders with the Italian Sentiment Lexicon"
n1 <- ggplot(sentences_female_cnr, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores - CNR",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
# Second plot
n2 <- ggplot(sentences_male_cnr, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores - NRC",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
(n1|n2)
sentences_active_cnr <- calculate_sentiment_score(data[data$active_users == 1,], Rep_Emoji, cnr_lex )
sentences_normal_cnr <- calculate_sentiment_score(data[data$active_users == 0,], Rep_Emoji, cnr_lex )
sentences_active_nrc <- calculate_sentiment_score(data[data$active_users == 1,], Rep_Emoji, nrc_lex )
sentences_normal_nrc <- calculate_sentiment_score(data[data$active_users == 0,], Rep_Emoji, nrc_lex )
#| fig-cap: "Sentences Sentiment between Genders with NRC"
hh1 <- ggplot(sentences_active_nrc, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores Female Users",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
# Second plot
hh2 <- ggplot(sentences_normal_nrc, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores Less Active Users",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
(hh1|hh2)
#| fig-cap: "Sentences Sentiment between Genders with the Italian Sentiment Lexicon"
nn1 <- ggplot(sentences_active_cnr, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores Most Active Users",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
# Second plot
nn2 <- ggplot(sentences_normal_cnr, aes(x = sign(sentiment_score), fill = factor(sign(sentiment_score)))) +
geom_bar() +
scale_fill_manual(
values = c("-1" = "red", "0" = "gray", "1" = "darkgreen"),
labels = c("-1" = "Negative", "0" = "Neutral", "1" = "Positive"),
name = "Sentiment"
) +
labs(
title = "Sentiment Scores Less Active Users",
x = "Sentiment Score",
y = "Number of Sentences"
) +
theme_minimal()
(nn1|nn2)
analyze_bigrams(corpus_result_erased$df_corpus)
analyze_bigrams <- function(df_corpus, min_count = 100, seed = 1) {
# Step 1: Create a mapping between stemmed words and original words
# First, tokenize all words and create a frequency table
word_tokens <- df_corpus %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
# Create stem-to-original mapping (keeping the most frequent original form)
stem_to_word <- word_tokens %>%
mutate(stem = wordStem(word, language = "italian")) %>%
group_by(stem) %>%
slice_max(n, n = 1, with_ties = FALSE) %>%
select(stem, word) %>%
ungroup()
# Step 2: Stemming (pre-processing)
df_stemmed <- df_corpus %>%
mutate(stemmed_text = sapply(df_corpus$text, function(x) {
# Tokenize the text
words <- unlist(strsplit(tolower(x), "\\s+"))
# Apply stemming
stemmed_words <- wordStem(words, language = "italian")
# Join back into text
paste(stemmed_words, collapse = " ")
}))
# Step 3: Extract bigrams
bigrams <- df_stemmed %>%
unnest_tokens(bigram, stemmed_text, token = 'ngrams', n = 2) %>%
separate(bigram, c('c1', 'c2'), sep = " ")
# Step 4: Calculate bigram counts
bigram_counts <- bigrams %>%
count(c1, c2, sort = TRUE) %>%
drop_na(c1) %>%
drop_na(c2)
# Step 5: Create bigram graph with original words
bigram_graph_data <- bigram_counts  %>%
# Join with stem-to-word mapping to get original words
left_join(stem_to_word, by = c("c1" = "stem")) %>%
rename(word1 = word) %>%
left_join(stem_to_word, by = c("c2" = "stem")) %>%
rename(word2 = word) %>%
# Use original words where available, fallback to stems if no mapping exists
mutate(
word1 = ifelse(is.na(word1), c1, word1),
word2 = ifelse(is.na(word2), c2, word2)
) %>%
select(word1, word2, n)
# Print top 10 bigrams
cat("Top 10 bigrams:\n")
print(head(bigram_graph_data, 10))
# Create graph from processed data
bigram_graph <- graph_from_data_frame(bigram_graph_data %>%
filter(n > min_count))
# Step 6: Create simplified bigram visualization
set.seed(seed)
# Make arrows longer
a <- grid::arrow(type = 'closed', length = unit(.15, 'inches'))
plot <- ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n),
show.legend = TRUE,
arrow = a,
end_cap = circle(.07, 'inches')) +
geom_node_point(color = "red", size = 3) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
labs(,
edge_alpha = "Co-occurrence frequency"
) +
theme_void() +
# Simplified legend - only for frequency
guides(edge_alpha = guide_legend(title = "Co-occurrence frequency")) +
theme(
legend.position = "right",
legend.title = element_text(size = 8),
legend.text = element_text(size = 7)
)
# Return results
results <- list(
bigram_graph_data = bigram_graph_data,
bigram_counts = bigram_counts,
bigram_graph = bigram_graph,
plot = plot,
stemmed_data = df_stemmed,
stem_to_word_mapping = stem_to_word
)
print(plot)
return(invisible(results))
}
analyze_bigrams(corpus_result_erased$df_corpus)
aa1 <- analyze_bigrams(corpus_result_erased_female$df_corpus)
aa2 <- analyze_bigrams(corpus_result_erased_male$df_corpus)
(aa1|aa2)
(aa1$plot|aa2$plot)
aa1 <- analyze_bigrams(corpus_result_erased_female$df_corpus, min = 50)
analyze_bigrams <- function(df_corpus, min_count = 100, seed = 1) {
# Step 1: Create a mapping between stemmed words and original words
# First, tokenize all words and create a frequency table
word_tokens <- df_corpus %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
# Create stem-to-original mapping (keeping the most frequent original form)
stem_to_word <- word_tokens %>%
mutate(stem = wordStem(word, language = "italian")) %>%
group_by(stem) %>%
slice_max(n, n = 1, with_ties = FALSE) %>%
select(stem, word) %>%
ungroup()
# Step 2: Stemming (pre-processing)
df_stemmed <- df_corpus %>%
mutate(stemmed_text = sapply(df_corpus$text, function(x) {
# Tokenize the text
words <- unlist(strsplit(tolower(x), "\\s+"))
# Apply stemming
stemmed_words <- wordStem(words, language = "italian")
# Join back into text
paste(stemmed_words, collapse = " ")
}))
# Step 3: Extract bigrams
bigrams <- df_stemmed %>%
unnest_tokens(bigram, stemmed_text, token = 'ngrams', n = 2) %>%
separate(bigram, c('c1', 'c2'), sep = " ")
# Step 4: Calculate bigram counts
bigram_counts <- bigrams %>%
count(c1, c2, sort = TRUE) %>%
drop_na(c1) %>%
drop_na(c2)
# Step 5: Create bigram graph with original words
bigram_graph_data <- bigram_counts  %>%
# Join with stem-to-word mapping to get original words
left_join(stem_to_word, by = c("c1" = "stem")) %>%
rename(word1 = word) %>%
left_join(stem_to_word, by = c("c2" = "stem")) %>%
rename(word2 = word) %>%
# Use original words where available, fallback to stems if no mapping exists
mutate(
word1 = ifelse(is.na(word1), c1, word1),
word2 = ifelse(is.na(word2), c2, word2)
) %>%
select(word1, word2, n)
# Print top 10 bigrams
cat("Top 10 bigrams:\n")
print(head(bigram_graph_data, 10))
# Create graph from processed data
bigram_graph <- graph_from_data_frame(bigram_graph_data %>%
filter(n > min_count))
# Step 6: Create simplified bigram visualization
set.seed(seed)
# Make arrows longer
a <- grid::arrow(type = 'closed', length = unit(.15, 'inches'))
plot <- ggraph(bigram_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n),
show.legend = TRUE,
arrow = a,
end_cap = circle(.07, 'inches')) +
geom_node_point(color = "red", size = 3) +
geom_node_text(aes(label = name), vjust = 0.5, hjust = 0.5) +
labs(,
edge_alpha = "Co-occurrence frequency"
) +
theme_void() +
# Simplified legend - only for frequency
guides(edge_alpha = guide_legend(title = "Co-occurrence frequency")) +
theme(
legend.position = "right",
legend.title = element_text(size = 8),
legend.text = element_text(size = 7)
)
# Return results
results <- list(
bigram_graph_data = bigram_graph_data,
bigram_counts = bigram_counts,
bigram_graph = bigram_graph,
plot = plot,
stemmed_data = df_stemmed,
stem_to_word_mapping = stem_to_word
)
print(plot)
return(invisible(results))
}
analyze_bigrams(corpus_result_erased$df_corpus)
aa1 <- analyze_bigrams(corpus_result_erased_female$df_corpus, min = 50)
aa2 <- analyze_bigrams(corpus_result_erased_male$df_corpus, min = 50)
(aa1$plot|aa2$plot)
bb1 <- analyze_bigrams(corpus_result_erased_active$df_corpus, min = 50)
bb2 <- analyze_bigrams(corpus_result_erased_normal$df_corpus, min = 50)
(bb1$plot|bb2$plot)
bb2 <- analyze_bigrams(corpus_result_erased_normal$df_corpus, min = 150)
(bb1$plot|bb2$plot)
bb2 <- analyze_bigrams(corpus_result_erased_normal$df_corpus, min = 100)
corpus_result_neg <- process_text_data(data, "Erase_Emoji", remove_words = setdiff(c(tm::stopwords('it'), "così"), "non"))
(bb1$plot|bb2$plot)
negation_words <- c('non', 'niente','nulla','nessuno', 'mai', 'neanche', 'neppure', 'nemmeno', 'senza')
score_unigrams <- get_sentiment(unique(corpus_result_neg$word_tokens$word), method = "nrc", language= "italian")
score_unigrams <- cbind(unique(corpus_result_neg$word_tokens$word), score_unigrams)
score_unigrams <- score_unigrams %>% as.data.frame() %>% mutate(sentiment = sign(as.numeric(score_unigrams)))
bigrams_neg <- corpus_result_neg$df_corpus %>%
unnest_tokens(bigram, text, token = 'ngrams', n = 2)  %>%
separate(bigram, c('c1', 'c2'), sep = " " )
negated_words <- bigrams_neg %>%
filter(c1 %in% negation_words) %>%
inner_join(score_unigrams, by = c('c2' = 'V1')) %>%
count(c1, c2, sentiment, sort = TRUE) %>%
ungroup()
top_neg_word <- negated_words %>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 5, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
ggplot(top_neg_word, aes(c2, n * sentiment, fill = n * sentiment > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
top_neg_word <- negated_words %>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 4, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
ggplot(top_neg_word, aes(c2, n * sentiment, fill = n * sentiment > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
top_neg_word <- negated_words %>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 3, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
ggplot(top_neg_word, aes(c2, n * sentiment, fill = n * sentiment > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
ggplot(top_neg_word, aes(c2, n * sentiment, fill = n * sentiment > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
top_neg_word <- negated_words %>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 5, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
```{r, fig.width= 10, fig.height= 7 }
ggplot(top_neg_word, aes(c2, n * sentiment, fill = n * sentiment > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
top_neg_word <- negated_words %>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 4, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
ggplot(top_neg_word, aes(c2, n * sentiment, fill = n * sentiment > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
# cnr
cnr_lex<- cnr_lex %>%
mutate(sentiment_num = case_when(
sentiment == "positive" ~ 1,
sentiment == "neutral" ~ 0,
sentiment == "negative" ~ -1,
TRUE ~ NA_real_ # opzionale per eventuali valori mancanti
))
negated_words_cnr <- bigrams_neg %>%
filter(c1 %in% negation_words) %>%
inner_join(cnr_lex, by = c('c2' = 'word')) %>%
count(c1, c2, sentiment_num, sort = TRUE) %>%
ungroup()
top_neg_word_cnr <- negated_words_cnr %>%
mutate(contribution = n * sentiment_num) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 5, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
ggplot(top_neg_word_cnr, aes(c2, n * sentiment_num, fill = n * sentiment_num > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
ggplot(top_neg_word_cnr, aes(c2, n * sentiment_num, fill = n * sentiment_num > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
top_neg_word_cnr <- negated_words_cnr %>%
mutate(contribution = n * sentiment_num) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 4, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
ggplot(top_neg_word_cnr, aes(c2, n * sentiment_num, fill = n * sentiment_num > 0)) +
geom_col(show.legend = FALSE) +
xlab("Words preceded by negation") +
ylab("Sentiment score * number of occurrences") +
facet_wrap(~c1, ncol = 2, scales = 'free') +
coord_flip()
warnings()
bigrams_neg <- corpus_result_neg$df_corpus %>%
unnest_tokens(bigram, text, token = 'ngrams', n = 2)  %>%
separate(bigram, c('c1', 'c2'), sep = " " )
bigrams_neg
corpus_result_neg <- process_text_data(data, "Erase_Emoji", remove_words = setdiff(c(tm::stopwords('it'), "così"), "non"))
corpus_result_neg <- process_text_data(data, "Erase_Emoji", remove_words = setdiff(c(tm::stopwords('it'), "così"), "non"))
negation_words <- c('non', 'niente','nulla','nessuno', 'mai', 'neanche', 'neppure', 'nemmeno', 'senza')
score_unigrams <- get_sentiment(unique(corpus_result_neg$word_tokens$word), method = "nrc", language= "italian")
score_unigrams <- cbind(unique(corpus_result_neg$word_tokens$word), score_unigrams)
score_unigrams <- score_unigrams %>% as.data.frame() %>% mutate(sentiment = sign(as.numeric(score_unigrams)))
bigrams_neg <- corpus_result_neg$df_corpus %>%
unnest_tokens(bigram, text, token = 'ngrams', n = 2)  %>%
separate(bigram, c('c1', 'c2'), sep = " " )
bigrams_neg <- corpus_result_neg$df_corpus %>%
unnest_tokens(bigram, text, token = 'ngrams', n = 2)  %>%
separate(bigram, c('c1', 'c2'), sep = " " )
negated_words <- bigrams_neg %>%
filter(c1 %in% negation_words) %>%
inner_join(score_unigrams, by = c('c2' = 'V1')) %>%
count(c1, c2, sentiment, sort = TRUE) %>%
ungroup()
top_neg_word <- negated_words %>%
mutate(contribution = n * sentiment) %>%
arrange(desc(abs(contribution))) %>%
group_by(c1) %>%
slice_max(n = 4, abs(contribution)) %>%
ungroup() %>%
mutate(c2 = reorder(c2, contribution))
